<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <!-- Prevent the browser from scaling down the entire page to fit the screen. See https://developer.mozilla.org/en-US/docs/Mozilla/Mobile/Viewport_meta_tag. -->
    <meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Eli's Blog</title>
    <link rel="stylesheet" href="/static/css/base.css">
    <link rel="stylesheet" href="/static/css/katex.css"> <!-- CSS for the rendered LaTeX. -->

    <script src="/static/js/lodash.min.js"></script>
    <script src="/static/js/jquery-2.2.2.js"></script>
    <script src="/static/js/katex.min.js"></script>
    <script src="/static/js/auto-render.min.js"></script>

    
<link rel="stylesheet" href="/static/css/post.css">

</head>

<body>
    <div class="container">
        <div class="header">
            <div>
                <a class="title" href="/index.html">Eli Rose</a> 's
                <a href="/index.html">posts</a>
                &sdot;
                <a href="/about.html">about</a>
                &sdot;
                <span class="dropdown">
                    <a href="javascript:void(0)">code trinkets</a>
                    <div class="dropdown-content">
                        <div>&sdot; <a href="/game_of_life.html">conway's game of life (in Elm)</a></div>
                        <div>&sdot; <a href="/turing.html">turing</a></div>
                        <div>&sdot; <a href="/sierpinski_fermat.html">sierpinksi's triangle and fermat numbers</a></div>
                        <div>&sdot; <a href="/fraction_pattern.html">fraction pattern</a></div>
                        <div>&sdot; <a href="/markov_text.html">literature scrambler</a></div>
                    </div>
                </span>
                &sdot;
                <a href="/quotes.html">quotes</a>
                &sdot;
                <a href="/media_log.html">media log</a>
            </div>

            <a href="/rss.xml">
                <img class="rss-icon" src="/static/images/feed-icon-28x28.png">
            </a>
        </div>

        <div id="content" class="content">
            
<div class="post-header">
    <div class="post-title">Really, Really Believing That Moral Realism Is False</div>
    <div class="post-date">Wednesday, October 30th, 2019</div>
</div>
<div>
    <p>Here is my understanding of how my morality works. My aim is to offer a descriptive account (how it <em>does</em> work, not how it <em>should</em> work), and to account for several puzzles. Amid some background discussion, I'll introduce the puzzles I aim to resolve in the first section, then provide my account in the second section.</p>
<h2>Puzzles</h2>
<p>I believe that moral realism is false, but I feel disturbed when I encounter people whose moral views seem to differ in fundamental ways from mine, and I want to convince other people to agree with me. For example, I don't think retributive justice is good for its own sake; I don't want people to suffer simply because they've committed crimes, even horrible ones. The only reason I would want this to happen is if I was convinced it was a price we somberly agreed to pay in service of some other goal, such as deterring others from committing those crimes.</p>
<p>I feel strongly that this is "the truth." What do I mean by that? I mean that I feel the same when trying to convince someone that the suffering of people who've committed crimes in not an ultimate good, but at best an instrumental good, as I do when trying to convince someone that the motion of molecules doesn't <em>cause</em> heat, but <em>is</em> heat. But, if I believe that moral realism is false (in other words, that there are no moral facts), what is that argument about? Is it completely confused? Does it have a chance of accomplishing anything?</p>
<p>The track record of these arguments is pretty bad. I've had (some) arguments which ended with someone adopting the other person's fundamental views on things like physics, but I've never had an argument that ended with the other person displacing one of my fundamental moral views to adopt mine. And likewise, I'm pretty open-minded, but I've never ended an argument by displacing one of my own fundamental moral views to adopt the other person's. If these arguments were completely confused philosophically, that would fit pretty well with the fact that they never get anywhere.</p>
<p>Yet, my own fundamental moral views haven't remained static throughout my life. They've changed with time (e.g. I became a vegetarian, I became more focused on the value of the future in a big-picture sense), and it seemed that arguing about morals with others was a component of this. I think it would be misleading to draw a straight line, and say "Well, I accepted this argument, and therefore changed this fundamental moral view." It would be misleading in the sense that that seems philosophically confused â€” how can moral views be reached through argument, if arguments are about facts and inferences, and there are no moral facts? But it would also be misleading in the sense of failing to descrieb what happened. My acceptance wasn't quick and directly attributable to one statement, like it was when someone first pointed out to me that since there's no air in space, spaceships wouldn't make sounds. It was gradual, rocky and frictionful, and felt more like being immersed in water than suddenly being splashed by it.</p>
<p><strong>First Puzzle</strong>: When I argue about fundamental moral views, if I'm not trying to discover truth, what am I doing?</p>
<p><strong>Second Puzzle</strong>: Can I ever succeed in changing my own or others' minds about fundamental moral views? How do fundamental moral views change, since evidently they do?</p>
<p>I'll make a case for the practical importance of these puzzles. It seems that many arguments about e.g. policy issues are in part arguments over fundamental moral differences. In his book <em><a href="https://en.wikipedia.org/wiki/The_Righteous_Mind">The Righteous Mind</a></em>, Jonathan Haidt says that liberals and conservatives have (on average) fundamental moral differences. Liberals care mostly about fairness and individual wellbeing, while conservatives care about those things as well as loyalty, respect for legitimate authority, and not profaning the sacred. We observe that policy debates between liberals and conservatives often feel unproductive and don't get anywhere, in just the way I discuss above. Yet people do become more liberal or more conservative with time.</p>
<p>Also, a meta-puzzle: if I'm committed to moral realism being false, why do I even want to convince other people of my moral views? I can imagine that my desire to convince other people of the truth about what heat is is motivated by the general idea that the truth is a good thing to spread around. But my desire to convince other people of the "truth" about retributive justice can't be motivated by this, because the "truth" is just my view, not distinguished among other views.</p>
<p>The easy answer is that I want to convince others instrumentally, because the more people who think that prisoners don't inherently deserve to suffer, the fewer prisoners will suffer. This makes sense. However, my brain doesn't have that plan. When I introspect, I see that my conscious mind has no specific plan of how spreading this belief will influence the world, they way it would if I were making a speech to a prison warden. It doesn't want to convince them of "the truth" because of what they'll do afterward, it wants to convince them of "the truth" because if they don't believe "the truth" then they are WRONG.</p>
<div style="text-align: center;">
  <img style="max-width: 100%;" src="/static/images/xkcd_wrong_on_the_internet.png"/>
  <div><a href="https://xkcd.com/386/">xkcd</a></div>
</div>

<p>As the comic indicates, I also care about this point way out of proportion to what would seem like an important variable, which is the chance that this person being convinced of my views will cause prisoners to be treated better. It doesn't matter who is wrong, just that someone is wrong. Actually, it does seem to matter who is wrong, but the thing that matters is not "How much power do they have?", it's more like "<a href="https://slatestarcodex.com/2014/09/30/i-can-tolerate-anything-except-the-outgroup/">How much are they part of my ingroup?</a>" For example, it feels to me like a big problem if me and my significant other disagree on, for example, the morality of the death penalty, even if my significant other is not in fact sitting on a jury in a murder case.</p>
<p><strong>Third Puzzle</strong>: Why do I try to convince others to share my fundamental moral views, even when it doesn't matter very much for any outcome? Why does it matter how much they are part of my ingroup?</p>
<h2>Account</h2>
<p>I have a set of preferences about how things should be. I visualize them as floating bubbles in the sea of my brain. Most of the preferences are things like "I want a banana right now," "I don't want her to say that to me," or "I want to get praise for this good job I did." These we do not call moral preferences.</p>
<p>I can have preferences about my preferences. These are meta-preferences. I'm bringing them up because they are related to morals, but not all are. An example is "I wish I was enjoying this party more." Meta-preferences can cause inner conflict because they are one part of the brain wanting something from the other part.</p>
<p>(I can also have preferences about other people's preferences; this can cause conflict of the garden variety.)</p>
<p>Moral preferences are a subset of preferences. They are preferences which are accompanied by some specific meta-preferences. For example, "I don't want him to go hungry, because no one should go hungry," "I want the children of this school to learn music, since music is the expression of God's joy in creation," or "I don't want to lie in this presentation, since lying is wrong," we do call moral. Usually, "X is wrong" is a clue that we're talking about a moral preference; consider the difference in meaning between "I don't want to do that" and "doing that would be wrong."</p>
<p>In the examples of moral preferences above, I included phrases beginning with "because" or "since". Those are the meta-preferences that are attached to the base preference. The thing that distinguishes moral preferences from non-moral preferences is that moral preferences always have certain meta-preferences attached. So a preference is like an atom, while a moral preference is like a molecule. Here are the meta-preferences that accompany any moral preference. I gave them all names that start with "S".</p>
<ul>
<li>Stability. A moral preference $P$ is accompanied by the meta-preference that $P$ be stable across time, place, context, and person. For example, "Lying is wrong" indicates the meta-preference "I want to want not to lie, no matter what the situation." Another name for this is fairness.</li>
<li>Sharedness. A moral preference $P$ is accompanied by the meta-preference that $P$ be shared by other people. For example, someone who thinks "Eating meat is wrong" is going to want to get others not to eat meat, and feel some desire not to associate with people who eat meat. The closer you feel to another person, the stronger your preference that they share $P$ with you.</li>
<li>Strength. A moral preference $P$ is accompanied by the meta-preference that $P$ be considered more important than other non-moral preferences. Between those (I want to go for a walk, but I also want to play basketball) it's possible to imagine parity. My preference to go on a walk might be stronger, or my preference to play basketball might be stronger, but it also might be that they are equally strong, and I end up flipping a coin to decide which one I do. Between a non-moral and a moral preference (I want to go for a walk, but I also want to help my friend move because friends should help one another), I would not be similarly indifferent to which one wins. Going for a walk might indeed win, but if it did I would experience some inner conflict, as both my preference to help my friend and my meta-preference to elevate helping my friends over other activities would be frustrated. Perhaps I rationalize until it seems to me that helping my friend move was not in fact an option, so my frustrated meta-preference no longer applies. I would not end up flipping a coin to decide; indifference is not characteristic of moral decisionmaking.</li>
</ul>
<p>These have a theme in common: universalizability. If a preference $P$ is moral, we want $P$ not to vary across time, place, context, according to person, according to whatever other preferences we feel concurrently, or even according to who believes it.</p>
<p>QUESTIONS:
  - How much are morals just viral memes? "I want other people to want the contents of this statement, including this bit."</p>
</div>
<div class="post-tags">
    
</div>

        </div>
    </div>

    <!-- Automatically render the LaTeX that appears in the page. -->
    <script>
        renderMathInElement(
            document.getElementById("content"), {
                delimiters: [
                    {left: "$$", right: "$$", display: true},
                    {left: "$", right: "$", display: false}
                ]
            }
        )
    </script>

</body>
</html>